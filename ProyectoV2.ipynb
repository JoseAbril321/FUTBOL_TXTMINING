{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video URL</th>\n",
       "      <th>Expanded_Event_Counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.youtube.com/watch?v=iyVECkH86Mw</td>\n",
       "      <td>{'goal': 66, 'yellow_card': 1, 'red_card': 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.youtube.com/watch?v=Iel1cl5aefk</td>\n",
       "      <td>{'goal': 13, 'yellow_card': 0, 'red_card': 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.youtube.com/watch?v=mUYahKeNf7E</td>\n",
       "      <td>{'goal': 67, 'yellow_card': 7, 'red_card': 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.youtube.com/watch?v=USNfMwo1i30</td>\n",
       "      <td>{'goal': 36, 'yellow_card': 4, 'red_card': 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.youtube.com/watch?v=8klVIK4Lwbc</td>\n",
       "      <td>{'goal': 24, 'yellow_card': 0, 'red_card': 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Video URL  \\\n",
       "0  https://www.youtube.com/watch?v=iyVECkH86Mw   \n",
       "1  https://www.youtube.com/watch?v=Iel1cl5aefk   \n",
       "2  https://www.youtube.com/watch?v=mUYahKeNf7E   \n",
       "3  https://www.youtube.com/watch?v=USNfMwo1i30   \n",
       "4  https://www.youtube.com/watch?v=8klVIK4Lwbc   \n",
       "\n",
       "                               Expanded_Event_Counts  \n",
       "0  {'goal': 66, 'yellow_card': 1, 'red_card': 1, ...  \n",
       "1  {'goal': 13, 'yellow_card': 0, 'red_card': 0, ...  \n",
       "2  {'goal': 67, 'yellow_card': 7, 'red_card': 1, ...  \n",
       "3  {'goal': 36, 'yellow_card': 4, 'red_card': 0, ...  \n",
       "4  {'goal': 24, 'yellow_card': 0, 'red_card': 0, ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Cargar el dataset (asegúrate de tener el archivo CSV en tu ruta)\n",
    "data = pd.read_csv('test.csv')\n",
    "\n",
    "# Función para limpiar los subtítulos\n",
    "def clean_subtitles(subtitles):\n",
    "    clean_text = re.sub(r'\\[.*?\\]', '', subtitles)  # Eliminar etiquetas como '[Aplausos]'\n",
    "    clean_text = re.sub(r'WEBVTT.*Language: es', '', clean_text)  # Eliminar metadatos\n",
    "    clean_text = clean_text.lower()  # Convertir a minúsculas\n",
    "    clean_text = re.sub(r'[^\\w\\s]', '', clean_text)  # Eliminar puntuación\n",
    "    return clean_text\n",
    "\n",
    "# Expresiones regulares para extraer eventos clave\n",
    "event_patterns = {\n",
    "    'goal': r'\\b(gol|marca|anota)\\b',\n",
    "    'yellow_card': r'\\b(tarjeta amarilla|amonestado|amonestar)\\b',\n",
    "    'red_card': r'\\b(tarjeta roja|expulsado|expulsion|expulsar)\\b',\n",
    "    'corner': r'\\b(tiro de esquina|corner)\\b',\n",
    "    'offside': r'\\b(fuera de juego|offside)\\b',\n",
    "    'foul': r'\\b(falta|infracción)\\b',\n",
    "    'shot': r'\\b(tiro|disparo)\\b',\n",
    "    'penalty': r'\\b(penal|penalty)\\b'\n",
    "}\n",
    "\n",
    "# Función para detectar eventos\n",
    "def detect_events(cleaned_text, event_patterns):\n",
    "    events_detected = {}\n",
    "    for event, pattern in event_patterns.items():\n",
    "        events_detected[event] = len(re.findall(pattern, cleaned_text))\n",
    "    return events_detected\n",
    "\n",
    "# Aplicación del proceso de limpieza y detección de eventos\n",
    "data['Cleaned_Subtitles'] = data['Subtitles'].apply(clean_subtitles)\n",
    "data['Expanded_Event_Counts'] = data['Cleaned_Subtitles'].apply(lambda text: detect_events(text, event_patterns))\n",
    "\n",
    "# Mostrar los resultados\n",
    "data[['Video URL', 'Expanded_Event_Counts']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Diego\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 375ms/step - loss: 309.4113 - mae: 9.7861 - val_loss: 272.9648 - val_mae: 9.1799\n",
      "Epoch 2/5\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 269ms/step - loss: 315.0133 - mae: 9.5821 - val_loss: 271.6274 - val_mae: 9.1418\n",
      "Epoch 3/5\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 256ms/step - loss: 287.6452 - mae: 9.3312 - val_loss: 264.0396 - val_mae: 8.9233\n",
      "Epoch 4/5\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 258ms/step - loss: 310.1055 - mae: 9.4119 - val_loss: 229.6618 - val_mae: 8.0625\n",
      "Epoch 5/5\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 252ms/step - loss: 237.8437 - mae: 8.2245 - val_loss: 211.1994 - val_mae: 7.7645\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 211.1994 - mae: 7.7645\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n",
      "                         Actual  \\\n",
      "0    [11, 3, 0, 0, 5, 0, 13, 5]   \n",
      "1     [23, 1, 1, 0, 7, 4, 6, 1]   \n",
      "2  [40, 3, 0, 2, 10, 51, 19, 2]   \n",
      "3  [41, 0, 0, 0, 17, 13, 24, 2]   \n",
      "4      [8, 2, 0, 0, 0, 3, 8, 3]   \n",
      "\n",
      "                                           Predicted  \n",
      "0  [5.312953948974609, 3.108689546585083, 1.49574...  \n",
      "1  [5.341035842895508, 3.122396945953369, 1.50729...  \n",
      "2  [5.347888946533203, 3.122718095779419, 1.50891...  \n",
      "3  [5.329665184020996, 3.118072986602783, 1.50117...  \n",
      "4  [5.341038703918457, 3.1276283264160156, 1.5125...  \n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Prepare text data and labels\n",
    "texts = data['Cleaned_Subtitles'].tolist()\n",
    "labels = data['Expanded_Event_Counts'].apply(lambda x: list(x.values())).tolist()\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Padding sequences for uniform input size\n",
    "max_sequence_length = 1000\n",
    "X = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Convert labels to a suitable format\n",
    "Y = pd.DataFrame(labels)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=10000, output_dim=128, input_length=max_sequence_length))\n",
    "model.add(LSTM(128, return_sequences=False))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(len(event_patterns), activation='linear'))  # Multi-output regression\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, Y_train, epochs=5, batch_size=32, validation_data=(X_test, Y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test, Y_test)\n",
    "\n",
    "# Predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Display predictions and actual values for comparison\n",
    "comparison_df = pd.DataFrame({'Actual': Y_test.values.tolist(), 'Predicted': predictions.tolist()})\n",
    "print(comparison_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
